import os
import json
import re
import math
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from functools import lru_cache
from typing import List

from langchain_chroma import Chroma
from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain_community.storage import RedisStore
from langchain_community.utilities.redis import get_client
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain.schema import Document

# =========================
# CONFIG
# =========================
load_dotenv()

REDIS_HOST = os.getenv("REDIS_HOST")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD")
REDIS_USERNAME = os.getenv("REDIS_USERNAME", "default")
REDIS_DB = int(os.getenv("REDIS_DB", 0))
REDIS_URL = f"redis://{REDIS_USERNAME}:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}/{REDIS_DB}"

CHROMA_DIR = "chroma_store"
COLLECTION_NAME = "summaries"

# Embeddings used elsewhere (compressor uses the same instance)
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

# =========================
# Redis + Chroma Init
# =========================
redis_client = get_client(REDIS_URL)
redis_store = RedisStore(client=redis_client, namespace="docs")

chroma = Chroma(
    collection_name=COLLECTION_NAME,
    embedding_function=embeddings,
    persist_directory=None
)

multi_retriever = MultiVectorRetriever(
    vectorstore=chroma,
    docstore=redis_store,
    id_key="doc_id"
)

vector_retriever = chroma.as_retriever(search_kwargs={"k": 40})

# =========================
# Custom Keyword Retriever
# =========================
class SimpleKeywordRetriever:
    def __init__(self, redis_store, k=20):
        self.redis_store = redis_store
        self.k = k

    def get_relevant_documents(self, query: str):
        results = []
        pattern = re.compile(re.escape(query), re.IGNORECASE)

        keys = self.redis_store.client.keys(f"{self.redis_store.namespace}:*")
        for key in keys:
            raw_json = self.redis_store.client.get(key)
            if not raw_json:
                continue
            doc = json.loads(raw_json)
            content = doc.get("content", "")
            if pattern.search(content):
                results.append(Document(page_content=content, metadata={"doc_id": key.decode().split(":")[-1]}))
                if len(results) >= self.k:
                    break
        return results

keyword_retriever = SimpleKeywordRetriever(redis_store, k=40)

# =========================
# Hybrid Retriever
# =========================
class HybridRetriever:
    def __init__(self, vector_retriever, keyword_retriever):
        self.vector_retriever = vector_retriever
        self.keyword_retriever = keyword_retriever

    def get_relevant_documents(self, query):
        vector_docs = self.vector_retriever.get_relevant_documents(query)
        keyword_docs = self.keyword_retriever.get_relevant_documents(query)

        merged, seen_ids = [], set()
        for d in vector_docs + keyword_docs:
            doc_id = d.metadata.get("doc_id")
            if doc_id and doc_id not in seen_ids:
                merged.append(d)
                seen_ids.add(doc_id)
        return merged

hybrid_retriever = HybridRetriever(vector_retriever, keyword_retriever)

# =========================
# Manual Compression
# =========================
compressor = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.6)

def compress_docs(query, docs):
    return compressor.compress_documents(docs, query=query)

# =========================
# Gemini
# =========================
gemini = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

# =========================
# Advanced (non-invasive) RAG enhancements
# - Re-rank compressed docs using embeddings similarity
# - Extract concise snippet (best sentence) per doc
# - Compute a simple confidence score
# =========================

def _cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    if a is None or b is None:
        return 0.0
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    if denom == 0:
        return 0.0
    return float(np.dot(a, b) / denom)

def embed_texts(texts: List[str]) -> List[np.ndarray]:
    """
    Use your embeddings object to embed documents.
    This expects the GoogleGenerativeAIEmbeddings instance to support .embed_documents
    (if different in your environment, swap to the correct method).
    """
    if not texts:
        return []
    # embeddings.embed_documents should accept list[str] and return list[list[float]]
    vecs = embeddings.embed_documents(texts)
    return [np.array(v, dtype=np.float32) for v in vecs]

def embed_query(query: str) -> np.ndarray:
    # embeddings.embed_query returns a list/array; wrap as numpy
    qvec = embeddings.embed_query(query)
    return np.array(qvec, dtype=np.float32)

def get_best_snippet(text: str, query: str, max_len=400) -> str:
    """
    Find the best sentence matching query terms; fallback to first max_len chars.
    """
    # split into sentences (simple)
    sentences = re.split(r'(?<=[.!?ã€‚\n])\s+', text.strip())
    terms = re.findall(r'\w+', query)
    if terms:
        term_pattern = re.compile("|".join(re.escape(t) for t in terms), re.IGNORECASE)
        # rank sentences by number of term matches and sentence length proximity
        scored = []
        for s in sentences:
            matches = len(term_pattern.findall(s))
            if matches > 0:
                scored.append((matches, s.strip()))
        if scored:
            # choose sentence with highest matches (and not too long)
            scored.sort(key=lambda x: (-x[0], len(x[1])))
            best = scored[0][1]
            # truncate if necessary
            return best if len(best) <= max_len else best[:max_len].rsplit(' ', 1)[0] + "..."
    # fallback: return the beginning of the document
    plain = re.sub(r'\s+', ' ', text).strip()
    return plain[:max_len].rsplit(' ', 1)[0] + ("..." if len(plain) > max_len else "")

def rerank_and_prepare(query: str, docs: List[Document], top_n: int = 5):
    """
    Re-rank compressed documents by embedding similarity to the query,
    add snippet and score metadata, and return top_n documents.
    """
    if not docs:
        return []

    texts = [d.page_content for d in docs]
    # compute embeddings
    qvec = embed_query(query)
    doc_vecs = embed_texts(texts)

    scores = []
    for i, dv in enumerate(doc_vecs):
        sim = _cosine_sim(qvec, dv)
        scores.append((i, sim))

    # sort by similarity descending
    scores.sort(key=lambda x: x[1], reverse=True)
    selected = []
    for idx, sim in scores[:top_n]:
        d = docs[idx]
        # keep original metadata, add score & snippet
        snippet = get_best_snippet(d.page_content, query)
        # copy doc (Document is a dataclass-like object) - create new Document preserving metadata
        metadata = dict(d.metadata or {})
        metadata["rerank_score"] = float(sim)
        metadata["snippet"] = snippet
        # preserve doc_id if it exists
        selected.append(Document(page_content=d.page_content, metadata=metadata))
    return selected

def compute_confidence_score(reranked_docs: List[Document]) -> float:
    """
    Simple confidence: average rerank_score normalized to [0,1], with weight for number of docs.
    """
    if not reranked_docs:
        return 0.0
    scores = [float(d.metadata.get("rerank_score", 0.0)) for d in reranked_docs]
    avg = float(np.mean(scores))
    # map avg (which is cosine similarity in [-1,1]) to [0,1]
    normalized = (avg + 1) / 2
    # weight by fraction of docs that have score > 0.1 (signal present)
    positive_frac = sum(1 for s in scores if s > 0.1) / len(scores)
    confidence = normalized * (0.6 + 0.4 * positive_frac)  # base 0.6 + up to 0.4
    return float(max(0.0, min(1.0, confidence)))

# =========================
# Query Function (keeps pipeline logic intact)
# =========================
def query_rag(query_text, top_k=20, rerank_top_k=5):
    # 1) retrieval (unchanged)
    retrieved_docs = hybrid_retriever.get_relevant_documents(query_text)

    # 2) compression (unchanged)
    compressed_docs = compress_docs(query_text, retrieved_docs)

    # 3) prepare raw docs and used_doc_ids (unchanged part kept, but we add re-ranking/snip)
    raw_docs, used_doc_ids = [], []
    for doc in compressed_docs:
        doc_id = doc.metadata.get("doc_id")
        if not doc_id:
            continue
        raw_json_list = redis_store.mget([doc_id])
        if raw_json_list and raw_json_list[0]:
            raw_doc = json.loads(raw_json_list[0])
            # keep original content inside Document for reranking/snippet stage
            # we attach full content into a Document instance for reranking step
            raw_docs.append(Document(page_content=raw_doc.get("content", ""), metadata={"doc_id": doc_id}))
            used_doc_ids.append(doc_id)

    if not raw_docs:
        return {"answer": "No relevant documents found.", "doc_ids": [], "confidence": 0.0, "used_snippets": []}

    # 4) rerank and extract snippets (new enhancement, does NOT replace compression/retrieval)
    reranked_docs = rerank_and_prepare(query_text, raw_docs, top_n=rerank_top_k)

    # 5) compute confidence
    confidence = compute_confidence_score(reranked_docs)

    # 6) build a concise context using snippets to reduce prompt noise
    context_parts = []
    used_snippets = []
    for i, d in enumerate(reranked_docs, start=1):
        doc_id = d.metadata.get("doc_id", f"doc_{i}")
        snippet = d.metadata.get("snippet", "")
        score = d.metadata.get("rerank_score", 0.0)
        # Each context includes a short snippet and an explicit [SOURCE: <id>] label
        part = f"[SOURCE: {doc_id} | score:{score:.4f}]\n{snippet}"
        context_parts.append(part)
        used_snippets.append({"doc_id": doc_id, "snippet": snippet, "score": float(score)})

    context_text = "\n\n---\n\n".join(context_parts[:top_k])

    # 7) improved system prompt but still strict: ask the model to cite sources and avoid hallucination
    system_prompt = (
        "You are a precise assistant. You must answer ONLY using the provided document snippets. "
        "For every factual claim, append the source tags in square brackets exactly as provided (e.g. [SOURCE: doc123]). "
        "Do not invent dates, numbers, or facts not present in documents."
    )

    user_prompt = f"Documents (snippets):\n{context_text}\n\nQuestion: {query_text}\n\nProvide a concise answer and cite sources."

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # 8) call Gemini exactly like before
    response = gemini.invoke(messages)

    # 9) return extended info (answer, doc ids, confidence, snippets)
    return {
        "answer": response.content,
        "doc_ids": used_doc_ids,
        "confidence": confidence,
        "used_snippets": used_snippets
    }

# =========================
# RAGAS imports (unchanged)
# =========================
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
from ragas import evaluate
from datasets import Dataset
from ragas.llms import LangchainLLMWrapper

# =========================
# Evaluation Function (unchanged logic, uses query_rag results)
# =========================
def evaluate_batch(qa_pairs):
    """Evaluate RAG system using RAGAS metrics"""
    records = []

    for q, expected in qa_pairs:
        result = query_rag(q, top_k=5, rerank_top_k=5)
        row = {
            "question": q,
            "answer": result["answer"],
            "contexts": result["doc_ids"],  # doc IDs instead of raw text to save space
            "ground_truth": expected
        }
        records.append(row)

    df = pd.DataFrame(records)
    dataset = Dataset.from_pandas(df)

    evaluator_llm = LangchainLLMWrapper(gemini)

    results = evaluate(
        dataset,
        metrics=[faithfulness, answer_relevancy, context_precision, context_recall],
        llm=evaluator_llm
    )

    df_results = results.to_pandas()
    df_results.to_csv("ragas_eval_results.csv", index=False, encoding="utf-8-sig")
    print("Evaluation complete. Saved to ragas_eval_results.csv")

    print("\nMetrics per example:")
    print(df_results[['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']])

    return df_results

# =========================
# QA Pairs for Evaluation (unchanged)
# =========================
qa_pairs = [
    ("ì´ë²ˆ ë‹¬ ìš°ë¦¬ íšŒì‚¬ ì „ì²´ ë§¤ì¶œì€ ì–¼ë§ˆì•¼?", "2025ë…„ 1ì›” ì‚¼ê´‘ Global ì „ì²´ ë§¤ì¶œì€ 335.4ì–µì›ì…ë‹ˆë‹¤. ì´ëŠ” ë‹¹ì´ˆ ì‚¬ì—…ê³„íš(213.4ì–µì›) ëŒ€ë¹„ 57% ì´ˆê³¼ ë‹¬ì„±í•œ ìˆ˜ì¹˜ì´ë©°, ì‹¤í–‰ê³„íš(307.8ì–µì›) ëŒ€ë¹„ë„ 109% ë‹¬ì„±í•œ ì„±ê³¼ì…ë‹ˆë‹¤."),
    ("ì‚¬ì—…ë¶€ë³„ ë§¤ì¶œ ë¹„ì¤‘ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", "2025ë…„ 1ì›” ê¸°ì¤€ ì‚¬ì—…ë¶€ë³„ ë§¤ì¶œ ë¹„ì¤‘ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\ní•œêµ­ ì‚¬ì—…ë¶€: 213.0ì–µì› (39.7%)\në² íŠ¸ë‚¨ ì‚¬ì—…ë¶€: 38.6ì–µì› (44.1%)\nì¸ë„ ì‚¬ì—…ë¶€: ë¯¸ë¯¸í•œ ìˆ˜ì¤€\nìœˆí…Œí¬: ë¯¸ë¯¸í•œ ìˆ˜ì¤€\n\ní•œêµ­ê³¼ ë² íŠ¸ë‚¨ ì‚¬ì—…ë¶€ê°€ ì „ì²´ ë§¤ì¶œì˜ ì•½ 84%ë¥¼ ì°¨ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤."),
    ("ìš°ë¦¬ íšŒì‚¬ ì˜ì—…ì´ìµë¥ ì€ ëª‡ %ì•¼?", "2025ë…„ 1ì›” ì „ì‚¬ ì˜ì—…ì´ìµë¥ ì€ 3%ì…ë‹ˆë‹¤. ì˜ì—…ì´ìµì€ 8.97ì–µì›ì´ë©°, ì‚¬ì—…ë¶€ë³„ë¡œëŠ” í•œêµ­ 4%, ë² íŠ¸ë‚¨ 2%, ìœˆí…Œí¬ëŠ” -7%ì˜ ì˜ì—…ì´ìµë¥ ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤."),
    ("TAB S10 ë„ì¥ ê³µì • ìˆ˜ìœ¨ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", "TAB S10 ì œí’ˆì˜ ë„ì¥ ê³µì • ìˆ˜ìœ¨ì€ í‰ê·  98%ë¡œ ë§¤ìš° ì–‘í˜¸í•©ë‹ˆë‹¤. ì„¸ë¶€ì ìœ¼ë¡œ TAB S10 REAR BODY ë„ì¥ì€ 98%, TAB S10 KNOB ë„ì¥ì€ 99%ì˜ ìˆ˜ìœ¨ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤."),
    ("ìµœê·¼ ìˆ˜ìœ¨ì´ ë‚®ì€ ê³µì •ì´ ìˆë‚˜ìš”?", "ë„¤, ëª‡ ê°€ì§€ ì£¼ì˜ê°€ í•„ìš”í•œ ê³µì •ì´ ìˆìŠµë‹ˆë‹¤:\n\nR47 ENCLOSURE, LOWER, BATTERY, LARGE ì‚¬ì¶œ: 59%\nR47 ARM, FRONT RIGHT, UPPER ì‚¬ì¶œ: 80%\nTab S10 FE FRONT BODY ì‚¬ì¶œ: 87%\n\nì´ ê³µì •ë“¤ì€ 90% ë¯¸ë§Œì˜ ìˆ˜ìœ¨ë¡œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."),
    ("ì‚¼ì„± í´ë”ë¸”í° ë¶€í’ˆ(SM-F ì‹œë¦¬ì¦ˆ) ìƒì‚° í˜„í™©ì€?", "ì‚¼ì„± í´ë”ë¸”í° ë¶€í’ˆ ìƒì‚°ì´ í™œë°œí•©ë‹ˆë‹¤:\n\nSM-F721U: FRONT DECO MAIN/SUB NC ê³µì • ìˆ˜ìœ¨ 96-97%\nSM-F731U: NC ê³µì • ìˆ˜ìœ¨ 97%, ì¡°ë¦½ ìˆ˜ìœ¨ 100%\nSM-F741U: NC ê³µì • ìˆ˜ìœ¨ 95%, ë ˆì´ì € ê³µì • ìˆ˜ìœ¨ 99%\nSM-F936U: NC ë° ì¡°ë¦½ ê³µì • ëª¨ë‘ 100% ìˆ˜ìœ¨ ë‹¬ì„±"),
    ("R47 ì‹œë¦¬ì¦ˆ ì¬ê³  í˜„í™©ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", "R47 ì‹œë¦¬ì¦ˆ ì£¼ìš” í’ˆëª© ì¬ê³  í˜„í™©:\n\nR47 ENCLOSURE, LOWER, BATTERY, LARGE ê°€ê³µí’ˆ: 568 EA (ì–‘í’ˆ)\nê¸°íƒ€ R47 ë¶€í’ˆë“¤ì€ í˜„ì¬ ì¬ê³ ê°€ ì—†ëŠ” ìƒíƒœì…ë‹ˆë‹¤.\nëŒ€ë¶€ë¶„ ê²Œì´íŠ¸ ì»¤íŒ… ê°€ê³µì´ë‚˜ ì‚¬ì¶œ ê³µì •ì„ ê±°ì¹˜ëŠ” ë¶€í’ˆë“¤ì…ë‹ˆë‹¤."),
    ("C18 ì œí’ˆêµ° ì¬ê³ ê°€ ìˆë‚˜ìš”?", "C18 ì œí’ˆêµ°ì€ ëª¨ë‘ ì¬ê³ ê°€ 0ì¸ ìƒíƒœì…ë‹ˆë‹¤. CLAMSHELL COVER, ENCLOSURE ë“± ì£¼ìš” ë¶€í’ˆë“¤ì´ ì¬ê³  ì†Œì§„ ìƒíƒœì´ë¯€ë¡œ ìƒì‚° ê³„íš ìˆ˜ë¦½ì´ í•„ìš”í•©ë‹ˆë‹¤."),
    ("ìš°ë¦¬ íšŒì‚¬ ë§¤ì¶œì›ê°€ìœ¨ì´ ë†’ì€ ì´ìœ ê°€ ë­ì•¼?", "2025ë…„ 1ì›” ì „ì‚¬ ë§¤ì¶œì›ê°€ìœ¨ì€ 92%ë¡œ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ì£¼ìš” ì›ì¸ì€:\n\në§¤ì…ë¹„(ì›ë¶€ìì¬+ì™¸ì£¼ê°€ê³µë¹„): 67% - ê°€ì¥ í° ë¹„ì¤‘\në…¸ë¬´ë¹„: 12%\nì œì¡°ê²½ë¹„: 11%\n\níŠ¹íˆ ë² íŠ¸ë‚¨ ì‚¬ì—…ë¶€(94%)ì™€ ì¸ë„ ì‚¬ì—…ë¶€(92%)ì˜ ë§¤ì¶œì›ê°€ìœ¨ì´ ë†’ì•„ ìˆ˜ìµì„± ê°œì„ ì´ ì‹œê¸‰í•©ë‹ˆë‹¤."),
    ("ì‹¤íŒ¨ë¹„ìš©ì´ ì–¼ë§ˆë‚˜ ë°œìƒí–ˆë‚˜ìš”?", "2025ë…„ 1ì›” ì „ì‚¬ ì‹¤íŒ¨ë¹„ìš©ì€ 5.16ì–µì›(ë§¤ì¶œ ëŒ€ë¹„ 2%)ì…ë‹ˆë‹¤. ì‚¬ì—…ë¶€ë³„ë¡œëŠ”:\n\ní•œêµ­: 0.23ì–µì› (1%)\në² íŠ¸ë‚¨: 3.95ì–µì› (2%) - ê°€ì¥ ë†’ìŒ\nì¸ë„: 0.48ì–µì› (1%)\nìœˆí…Œí¬: 0.50ì–µì› (1%)\n\në² íŠ¸ë‚¨ ì‚¬ì—…ë¶€ì˜ ì‹¤íŒ¨ë¹„ìš© ì ˆê°ì´ í•„ìš”í•©ë‹ˆë‹¤."),
    ("SMF741UB6 ì¡°ë¦½ ì‘ì—… ì‹œ ì£¼ì˜ì‚¬í•­ì´ ë­ì•¼?", "SMF741UB6 FRONT DECO SUB ì¡°ë¦½ ì‘ì—…í‘œì¤€ì„œì— ë”°ë¥¸ ì£¼ìš” ì£¼ì˜ì‚¬í•­ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. 2024ë…„ 7ì›” 8ì¼ì— ì¡°ë¦½ ë¶€ë¶„ì´ ìˆ˜ì •ëœ ìµœì‹  ë²„ì „ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."),
    ("ì´ë²ˆ ë‹¬ ìƒì‚°ì„±ì´ ê°€ì¥ ì¢‹ì€ ê³µì •ì€?", "ë‹¤ìŒ ê³µì •ë“¤ì´ 100% ìˆ˜ìœ¨ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤:\n\nSM-F936U NC ë° ì¡°ë¦½ ê³µì •\nC18 SHIM ê°€ê³µ ë° ì‚¬ì¶œ\nPA3 DECO ì•„ë…¸ë‹¤ì´ì§•, ìƒŒë”©, ë²„í•‘\nëŒ€ë¶€ë¶„ì˜ ì¡°ë¦½(ASS'Y) ê³µì •\n\nì´ë“¤ ê³µì •ì€ ë²¤ì¹˜ë§ˆí‚¹ ëŒ€ìƒìœ¼ë¡œ ì‚¼ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
]

# =========================
# CLI Menu (unchanged UX)
# =========================
if __name__ == "__main__":
    print("\nChoose an option:")
    print("1 - Ask a Question")
    print("2 - Evaluate RAG with RAGAS")
    choice = input("Enter choice (1/2): ").strip()

    if choice == "1":
        user_q = input("\nEnter your question: ")
        result = query_rag(user_q)
        print("\nâœ… Answer from Gemini:")
        print(result["answer"])
        print(f"\nğŸ” Confidence: {result['confidence']:.3f}")
        print("\nğŸ“„ Doc IDs used:")
        print(result["doc_ids"])
        print("\nâœ‚ï¸ Used snippets:")
        for s in result["used_snippets"]:
            print(f"- {s['doc_id']} (score={s['score']:.4f}): {s['snippet'][:200]}")

    elif choice == "2":
        print("\nğŸ” Running RAGAS evaluation...")
        evaluate_batch(qa_pairs)

    else:
        print("âŒ Invalid choice. Please select 1 or 2.")
